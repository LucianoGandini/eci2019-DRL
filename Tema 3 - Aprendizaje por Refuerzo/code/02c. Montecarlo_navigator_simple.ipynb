{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método de Montecarlo, versión simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Juan Gómez Romero**](https://decsai.ugr.es/~jgomez)  \n",
    "Departamento de Ciencias de la Computación e Inteligencia Artificial  \n",
    "Universidad de Granada  \n",
    "This work is licensed under the [GNU General Public License v3.0](https://choosealicense.com/licenses/gpl-3.0/).\n",
    "\n",
    "---\n",
    "\n",
    "Ejemplo basado en una versión simplificada del [robot navegador](https://github.com/jgromero/eci2019-DRL/blob/master/Tema%203%20-%20Aprendizaje%20por%20Refuerzo/Aprendizaje%20por%20refuerzo.pdf) del curso.\n",
    "\n",
    "Código adaptado de:\n",
    "> Udacity (2019) Deep Reinforcement Learning Course. Available in [GitHub](https://github.com/udacity/deep-reinforcement-learning/tree/master/monte-carlo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método de Montecarlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo\n",
    "A continuación se proporciona una implementación del algoritmo de Montecarlo para el robot navegador simplificado.\n",
    "\n",
    "Se generan dos episodios manualmente.\n",
    "\n",
    "![](navigator_simple.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se requiere [`pandas`](https://pandas.pydata.org) y [`tabulate`](https://pypi.org/project/tabulate/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/jgomez/anaconda3/envs/eci-drl/lib/python3.7/site-packages (0.25.0)\n",
      "Requirement already satisfied: tabulate in /Users/jgomez/anaconda3/envs/eci-drl/lib/python3.7/site-packages (0.8.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/jgomez/anaconda3/envs/eci-drl/lib/python3.7/site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/jgomez/anaconda3/envs/eci-drl/lib/python3.7/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/jgomez/anaconda3/envs/eci-drl/lib/python3.7/site-packages (from pandas) (1.16.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jgomez/anaconda3/envs/eci-drl/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
      "\u001b[33mYou are using pip version 18.0, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "\n",
    "def generate_custom_episode(ep_number=1):\n",
    "    \"\"\"Generar episodio personalizado:\n",
    "    Params\n",
    "    ======\n",
    "    ep_number: numero de episodio a generar (1, 2)\n",
    "    \"\"\"\n",
    "    episode = []\n",
    "    \n",
    "    if ep_number==1:\n",
    "        episode = [\n",
    "            ((1, 0), 0, -5),\n",
    "            ((0, 0), 1, +50)\n",
    "        ]\n",
    "    else:\n",
    "        episode = [\n",
    "            ((1, 0), 1, -1),\n",
    "            ((1, 1), 3, -1),\n",
    "            ((1, 0), 1, -1),\n",
    "            ((1, 1), 0, +50),\n",
    "        ]\n",
    "    \n",
    "    return episode\n",
    "\n",
    "def print_Q_table(Q):\n",
    "    \"\"\"Imprimir tabla Q\"\"\"  \n",
    "    Q_df = pd.DataFrame.from_dict(Q)\n",
    "    Q_df = Q_df.reindex(sorted(Q_df.columns), axis=1)\n",
    "    print(tabulate(Q_df.T, headers='keys'))\n",
    "    \n",
    "def mc_prediction_q(env, num_episodes, generate_episode, gamma=1.0):\n",
    "    \"\"\"Algoritmo de Montecarlo:\n",
    "    Params\n",
    "    ======\n",
    "    env: entorno, definido como estructura de datos\n",
    "    num_episodes: numero de episodios para explorar\n",
    "    generate_episode: funcion para generar episodios\n",
    "    gamma: descuento\n",
    "    \"\"\"\n",
    "\n",
    "    # inicializar diccionarios (prediccion every-visit)\n",
    "    Q = defaultdict(lambda: np.zeros(env['action_space.n']))  # Q\n",
    "    N = defaultdict(lambda: np.zeros(env['action_space.n']))  # numero de visitas a (estado, accion)\n",
    "    returns_sum = defaultdict(lambda: np.zeros(env['action_space.n'])) # suma de recompensa en (estado, accion)\n",
    "    \n",
    "    # bucle de episodios\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        \n",
    "        # generar episodio\n",
    "        episode = generate_episode(i_episode)\n",
    "        \n",
    "        # actualizar tabla Q\n",
    "        # - obtener estados, acciones y recompensas del episodio por separado\n",
    "        states, actions, rewards = zip(*episode)                \n",
    "        # - obtener gamma para aplicar descuentos\n",
    "        discounts = np.array([gamma**i for i in range(len(rewards)+1)])        \n",
    "        # - actualizar suma de recompensa, numero de visitas y Q para cada (estado, accion) del episodio        \n",
    "        for i, state in enumerate(states):\n",
    "            returns_sum[state][actions[i]] += sum(rewards[i:]*discounts[:-(1+i)])\n",
    "            N[state][actions[i]] += 1.0\n",
    "            Q[state][actions[i]] = returns_sum[state][actions[i]] / N[state][actions[i]]\n",
    "            \n",
    "        # imprimir tabla Q\n",
    "        print(\"\\nEpisodio {:d}:\".format(i_episode))\n",
    "        print_Q_table(Q)\n",
    "    \n",
    "    policy = dict((k,np.argmax(v)) for k, v in Q.items())\n",
    "            \n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para llamar al algoritmo, creamos el entorno y llamamos a la función `mc_prediction_q`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episodio 1:\n",
      "          0    1    2    3\n",
      "------  ---  ---  ---  ---\n",
      "(0, 0)    0   50    0    0\n",
      "(1, 0)   40    0    0    0\n",
      "\n",
      "Episodio 2:\n",
      "          0      1    2     3\n",
      "------  ---  -----  ---  ----\n",
      "(0, 0)    0  50       0   0\n",
      "(1, 0)   40  38.87    0   0\n",
      "(1, 1)   50   0       0  38.6\n"
     ]
    }
   ],
   "source": [
    "# crear entorno\n",
    "env = {\n",
    "    'action_space.n' : 4\n",
    "}\n",
    "\n",
    "# obtener estimacion de Q (funcion accion-valor)\n",
    "Q, policy = mc_prediction_q(env, 2, generate_custom_episode, 0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
